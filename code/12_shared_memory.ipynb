{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Memory Systems\n",
    "\n",
    "* Large class defined by memory model\n",
    "  * And thus, the programming model\n",
    "* In shared-memory programming, \n",
    "  * parallel _threads_ \n",
    "  * exchange information through reads and writes to shared memory.\n",
    "  * synchronization constructs  control sharing\n",
    "* Easy to use abstraction\n",
    "* Examples\n",
    "  * OpenMP, Java, pthreads, C++ threads\n",
    "  \n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/uma.gif\" width=\"512\" title=\"Uniform Shared Memory System\" />\n",
    "\n",
    "### SMP = Symmetric Multiprocess\n",
    "\n",
    "* Simplest form of shared-memory MIMD system in which \n",
    " * All processors can address all memory\n",
    " * Symmetric performance (latency and throughput) from all processors to all memory addresses\n",
    "* SMPs have scaling limits\n",
    "  * physics makes it difficult to maintain symmetry as we add cores to processors or processors to machines.\n",
    "  \n",
    "Few (or no) machines end up being symmetric in practice.  Let's consider:\n",
    "  * a two processor machine with symmetric access to memory\n",
    "  * each processor has 4-cores with symmetric access to an L3 cache\n",
    "As the program runs data ends up cached in memory, L3 and higher-level caches.  Processors have assymetric access to data in an L3 cache.  Cores have assymetric access to data in core-private caches.\n",
    "\n",
    "Highly-optimized programs must consider locality between compute and data even in \"symmetric\" architectures.\n",
    "\n",
    "### NUMA Machines\n",
    "\n",
    "* Shared memory MIMD systems\n",
    "  * Latency and bandwidth to physical memory differs by address and location\n",
    "* Same programming semantics as SMP\n",
    "\n",
    "The big hazard of shared memory programming is that tools (OpenMP, pthreads, Java threads) and programmers typically treat machines as if they were SMPs when in fact they are all NUMA.  Alas, programming for NUMA is hard.\n",
    "\n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/numa.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distributed Memory Systems\n",
    "These are MIMD machines in which the processing units have their own private memory.  The only way to share data among nodes is to moved it explicityly from one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
