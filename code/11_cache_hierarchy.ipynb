{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cache Hierarchy\n",
    "\n",
    "* Memory is an abstraction\n",
    "  * looks to processor like a 1-d adress space of data locations* \n",
    "  * uniform access from all cores/processors\n",
    "*  Actually a steep, hieararchy of cache in which different levels have different:\n",
    "  * Performance\n",
    "  * Capacity\n",
    "  * Sharing\n",
    "  \n",
    "<img src=\"https://sites.google.com/site/cachememory2011/_/rsrc/1311628836036/memory-hierarchy/hei.png\" width=512 title=\"Cache Hierarchy\" />\n",
    "\n",
    "* Caches are a place to store a smaller amount of data the is frequently/recently used to make data access faster.\n",
    "  * Processor caches (on chip) cache for memory.  Managed by hardware.\n",
    "  * Memory (DRAM) is a cache for pages from disk.  Managed by a storage system (database, file system).\n",
    "  * Management refers to the process of loading and evicting the contents in response to workload.\n",
    "  \n",
    "### The Hierarchy\n",
    "\n",
    "<img src=\"http://www.imexresearch.com/newsletters/images/201009_SSDImages/20100913_SSD_0000.png\" width=512 title=\"IMEX Data on Latency and Cost\" />\n",
    "\n",
    "<img src=\"https://eda360insider.files.wordpress.com/2012/05/wegener-1.gif?w=1400\" width=512 title=\"Cache latency and granularity\" />\n",
    "\n",
    "### Latency\n",
    "\n",
    "Delays (in clock cycles) to different levels in the cache hierarchy for an i7 (Nehalem)\n",
    " * $1$ cycle to registers (private to each core)\n",
    " * $1$ cycle to L1 (private to each core)\n",
    " * $4$ cycles to L2 (private to each core)\n",
    " * $35$ cycles to L3 (shared by cores)\n",
    " * $145$ cycles to memory (shared by processors)\n",
    " * $10^5$ cycles to NVRAM\n",
    " * $10^7$ cycles to disk\n",
    "\n",
    "_Data Loading_: New data that has not been used yet must come from SSD or disk.  Can be very slow.\n",
    "\n",
    "_Data Sharing_: When two threads need to share data, they incur the cost of transferring data through the fastest shared cache.\n",
    "  * 2 cores on the same processor take 70 cycles (35 to write to L3 and 35 to read from L3)\n",
    "  * 2 processors take 290 cycles\n",
    "Sharing dyanmics result in _interference_ between processes that share data in OpenMP and threads.  This is the major source for lost parallelism in these programming models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor Caching Concepts\n",
    "\n",
    "* __cache line__: data are moved among levels in the cache one line at a time\n",
    "  * as small as 64 bytes (L1 or L2)\n",
    "  * think of each load as the parallel load of an entire line\n",
    "  * good parallel programs will use as 64 bytes\n",
    "* __unified__: refers to whether or not the cache is shared (among cores or processors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xeus-cling-cpp11"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "-std=c++11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
