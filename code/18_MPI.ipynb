{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI: Message Passing Interface\n",
    "\n",
    "* Message passing parallelism\n",
    "* Cluster computing (no shared memory)\n",
    "* Process (not thread oriented)\n",
    "* Parallelism model\n",
    "  * SPMD: by definition\n",
    "* MPI environment\n",
    "  * Application programming interface\n",
    "  * Implemented in libraries\n",
    "  * Multi-language support (C/C++ and Fortran)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPMD: Single program multiple data\n",
    "\n",
    "From wikipedia “Tasks are split up and run simultaneously on multiple processors with different input in order to obtain results faster. SPMD is the most common style of parallel programming.”\n",
    "  * Asynchronous execution of the same program (unlike SIMD)\n",
    "  \n",
    "<img src=\"https://www.sharcnet.ca/help/images/8/8a/SPMD_model.png\" width=512 title=\"SPMD\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first MPI program\n",
    "\n",
    "* Configure the MPI environment\n",
    "* Discover yourself\n",
    "* Take some differentiated activity\n",
    "\n",
    "all demos in `./mpi` Start with `mpimsg.c`\n",
    "\n",
    "* Idioms\n",
    "  * SPMD: all processes run the same program\n",
    "    * MPI_Rank: tell yourself apart from other and customize the local processes behaviours\n",
    "    * Find neighbors, select data region, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Vision circa 1996 (Poster at Supercomputing)\n",
    "\n",
    "<img src=\"https://www.netlib.org/mpi/mpi.gif\" width=512 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MPI Toolchain\n",
    "\n",
    "Build and launch scripts that wrap a compiler.\n",
    "    \n",
    "<img src=\"./images/mpiscip.png\" width=400 />\n",
    "\n",
    "* To compile an MPI program, you call the associated wrapper.\n",
    "* To run an MPI program:\n",
    "  * **debug** `mpirun` to launch MPI job on the local machine/cluster\n",
    "  * **deploy** launch through scheduler on HPC clusters (do not run on the login node)\n",
    "\n",
    "    \n",
    "```\n",
    "mpicc mpimsg.c -o mpimsg\n",
    "mpirun mpimsg\n",
    "mpirun -np 16 --oversubscribe mpimsg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HPC Scheduler\n",
    "\n",
    "Schedule many paralllel jobs onto a supercomputer based on size, resources needed, priority.\n",
    "* Maui/Torque\n",
    "* SLURM\n",
    "* OGE\n",
    "Each with their own submission scripts. Not mpirun.\n",
    "    \n",
    "<img src=\"http://docs.alces-flight.com/en/stable/_images/tetris.jpg\" width=512 />\n",
    "    \n",
    "HPC systems have login nodes that you `ssh` into.  **Do not call `mpirun`** on l\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xeus-cling-cpp11"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "-std=c++11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
